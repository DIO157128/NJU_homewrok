# BERT实验报告

在此次实验中，为了达到更好的情感分析预测结果，我们使用了Roberta模型。同时，我们在Roberta模型上额外增加了一些层来提高分类任务的表现。

## BERT与Roberta

BERT是由Google研究团队于2018年提出的一种预训练语言模型。相比于以往的语言模型，BERT引入了双向（bidirectional）训练机制，能够同时利用上下文信息进行预测任务。BERT模型通常使用了一个双向Transformer编码器，该编码器能够学习输入文本中的上下文信息，生成高质量的文本表示。BERT在多项NLP任务中表现出色，如文本分类、命名实体识别、问答系统等。

RoBERTa是由Facebook研究团队在2019年改进BERT模型的基础上提出的。RoBERTa的目标是通过更好的训练策略和超参数调整来进一步提高模型性能。相较于BERT，RoBERTa使用更大的模型规模和更长的训练时间，并对训练数据进行了增强。RoBERTa在各种NLP任务中展现了比BERT更好的性能。

在决定使用语言预训练模型时，我们最初考虑使用BERT。同时，为了优化实验结果，我们没有使用BERTForSequentialClassification，而是自行包装了一个类，具体来说，我们在BERTModel的基础上，额外增加一个线性层，一个Relu激活函数，具体代码如下：

```python
class BertClassifier(nn.Module):
    def __init__(self, ):
        """
        freeze_bert (bool): 设置是否进行微调，0就是不，1就是调
        """
        super(BertClassifier, self).__init__()
        # 输入维度(hidden size of Bert)默认768，分类器隐藏维度，输出维度(label)
        D_in, H, D_out = 768, 100, 3

        # 实体化Bert模型
        self.bert = BertModel.from_pretrained('bert-base-uncased')

        # 实体化一个单层前馈分类器，说白了就是最后要输出的时候搞个全连接层
        self.classifier = nn.Sequential(
            nn.Linear(D_in, H),  # 全连接
            nn.ReLU(),  # 激活函数
            nn.Linear(H, D_out)  # 全连接
        )

    def forward(self, input_ids, attention_mask):
        # 开始搭建整个网络了
        # 输入
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask)
        # 为分类任务提取标记[CLS]的最后隐藏状态，因为要连接传到全连接层去
        last_hidden_state_cls = outputs[0][:, 0, :]
        # 全连接，计算，输出label
        logits = self.classifier(last_hidden_state_cls)

        return logits
```

但是，使用BERT模型的效果并不理想，最佳情况也只有88%左右。所以，为了是结果更加完善，我们使用了Roberta模型。同时在输出层增加了lstm,cnn等层来提升输出结果。具体代码如下：

```python
class BertClassifier(nn.Module):
    def __init__(self, ):
        """
        freeze_bert (bool): 设置是否进行微调，0就是不，1就是调
        """
        super(BertClassifier, self).__init__()
        # 输入维度(hidden size of Bert)默认768，分类器隐藏维度，输出维度(label)
        D_in, H, D_out = 768, 100, 3

        # 实体化Bert模型
        # self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.bert = RobertaModel.from_pretrained('roberta-base')
        
        # Add a MultiheadAttention layer
        self.attention = nn.MultiheadAttention(D_in, num_heads=1)
        
        # Add a CNN layer
        self.cnn = nn.Conv1d(D_in, H, kernel_size=3)
        
        
        # Add a LSTM layer
        self.lstm = nn.LSTM(H, H//2, batch_first=True)
        
        # 实体化一个单层前馈分类器，说白了就是最后要输出的时候搞个全连接层
        self.classifier = nn.Sequential(
            nn.Linear(H//2, H//4),  # 全连接
            nn.BatchNorm1d(H//4),
            nn.ReLU(),  # 激活函数
            nn.Dropout(0.2), # Dropout
            nn.Linear(H//4, D_out),  # 全连接
            nn.Softmax(dim=1) #这是一个softmax层，它可以用来把每个类别的得分转换成概率，方便计算损失函数和评估指标。
        )

    def forward(self, input_ids, attention_mask):
        # 开始搭建整个网络了
        # 输入
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask)
                            
        # Use the last hidden state output from BERT encoder
        last_hidden_state = outputs[0]
        
         # Apply attention to the BERT output
        attn_output, _ = self.attention(last_hidden_state.transpose(0,1), last_hidden_state.transpose(0,1), last_hidden_state.transpose(0,1))
        attn_output = attn_output.transpose(0,1)
        
        # Apply CNN layer to the hidden states
        cnn_output = self.cnn(attn_output.transpose(1,2))
        
        
        # Apply LSTM layer to the CNN output
        lstm_output, _ = self.lstm(cnn_output.transpose(1,2))
        
        # Get the last output from LSTM
        lstm_last_output = lstm_output[:, -1, :]

        logits = self.classifier(lstm_last_output)


        return logits

```

在向前传播的过程中，词向量先经过Roberta模型输出，得到一个隐藏层状态。然后经过多头注意力曾提取注意力向量，然后依次经过cnn，lstm曾是的输出，最后经过一个前馈分类器，将结果转为一个三分类的结果。最后计算这个三维向量中较大值所在的index，即是最后的分类结果。经过实验，我们发现，这样做可以使分类准确率提升至90.42%

## 实验结果

最终，我们的Roberta分类模型的混淆矩阵输出如下：

需要注意的是，为了使Roberta模型能够理解分类标签。我们将积极，中性和消极分别转为数字0，1，2

|      | 0    | 1    | 2    |
| ---- | ---- | ---- | ---- |
| 0    | 319  | 39   | 2    |
| 1    | 37   | 444  | 27   |
| 2    | 1    | 21   | 436  |

|           | 0      | 1      | 2      |
| --------- | ------ | ------ | ------ |
| recall    | 0.8861 | 0.8740 | 0.9520 |
| precision | 0.8936 | 0.8810 | 0.9376 |
| f1-score  | 0.8898 | 0.8775 | 0.9447 |

总体的accuracy为0.9042